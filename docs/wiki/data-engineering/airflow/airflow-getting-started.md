---
title: "Airflow Getting Started"
parent: Airflow
grand_parent: Data Engineering
permalink: /de/airflow/getting-started
---

## 1. Airflowë€?

> **Airflow = ì‘ì—…ë“¤ì„ ìë™ìœ¼ë¡œ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•´ì£¼ëŠ” ìŠ¤ì¼€ì¤„ëŸ¬ë¡œ ì‘ì—…ì˜ ìˆœì„œì™€ ì‹¤í–‰ì„ ê´€ë¦¬í•¨**

---

## 2. í•µì‹¬ ìš©ì–´ 4ê°€ì§€

### 2.1 DAG (ëŒ€ê·¸) - ì‘ì—… ì„¤ê³„ë„

**DAG = Directed Acyclic Graph**

> ğŸ’¡ **DAG = "A í•˜ê³  â†’ B í•˜ê³  â†’ C í•´" ì´ëŸ° ì‘ì—… íë¦„ì„ ì •ì˜í•œ íŒŒì¼**
**ì˜ˆì‹œ:** ë°ì´í„° ìˆ˜ì§‘ â†’ ë°ì´í„° ì •ì œ â†’ ë¶„ì„ â†’ ë¦¬í¬íŠ¸ ìƒì„±

### 2.2 Task (íƒœìŠ¤í¬) - ì‹¤ì œ í•  ì¼

> âš™ï¸ **Task = DAG ì•ˆì— ìˆëŠ” ê°ê°ì˜ ì‘ì—… í•˜ë‚˜**
ìœ„ ì˜ˆì‹œì— ë‚˜ì˜¨ "ë°ì´í„° ìˆ˜ì§‘", "ë°ì´í„° ì •ì œ", "ë¶„ì„", "ë¦¬í¬íŠ¸ ìƒì„±" ê°™ì€ ê°œë³„ ë‹¨ê³„ë¥¼ ì˜ë¯¸

### 2.3 Operator (ì˜¤í¼ë ˆì´í„°) - ì‘ì—… ìœ í˜•

Taskë¥¼ **ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ** ì‹¤í–‰í• ì§€ ì •í•˜ëŠ” ê²ƒ:

| Operator | ìš©ë„ | ì‰¬ìš´ ì„¤ëª… |
|----------|------|-----------|
| BashOperator | ì‰˜ ëª…ë ¹ì–´ ì‹¤í–‰ | í„°ë¯¸ë„ì— ëª…ë ¹ ì¹˜ëŠ” ê²ƒ |
| PythonOperator | Python í•¨ìˆ˜ ì‹¤í–‰ | Python ì½”ë“œ ëŒë¦¬ê¸° |
| EmailOperator | ì´ë©”ì¼ ë°œì†¡ | ìë™ ë©”ì¼ ë³´ë‚´ê¸° |
| SQLOperator | SQL ì¿¼ë¦¬ ì‹¤í–‰ | DBì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° |

> BaseOperator ë¥¼ ìƒì†ë°›ì•„ CustomOperatorë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŒ (ì›¬ë§Œí•˜ë©´ PythonOperatorë¡œ ì»¤ë²„ê°€ ê°€ëŠ¥í•´ì„œ ë§ì´ ì‚¬ìš©ë˜ì§„ ì•ŠìŒ)

### 2.4 Schedule (ìŠ¤ì¼€ì¤„) - ì–¸ì œ ì‹¤í–‰?

> â° **Schedule = DAGì„ ì–¸ì œ ì‹¤í–‰í• ì§€ ì •í•˜ëŠ” ê²ƒ**

ìì£¼ ì“°ëŠ” ìŠ¤ì¼€ì¤„ í‘œí˜„:

| í‘œí˜„ | ì˜ë¯¸ |
|------|------|
| `@daily` | ë§¤ì¼ ìì • |
| `@hourly` | ë§¤ì‹œê°„ |
| `@weekly` | ë§¤ì£¼ ì¼ìš”ì¼ ìì • |
| `0 9 * * *` | ë§¤ì¼ ì˜¤ì „ 9ì‹œ (cron) |

---

## 3. Airflow êµ¬ì¡°

### ì£¼ìš” êµ¬ì„± ìš”ì†Œ

| êµ¬ì„±ìš”ì†Œ | ì—­í•  | ë¹„ìœ  |
|----------|------|------|
| **Scheduler** | DAGì„ ëª¨ë‹ˆí„°ë§í•˜ê³  ì‹¤í–‰ ì‹œì ì´ ë˜ë©´ Workerì—ê²Œ ì‘ì—… í• ë‹¹ | íšŒì‚¬ì˜ ê´€ë¦¬ì/ë§¤ë‹ˆì € |
| **Worker** | ì‹¤ì œ Taskë¥¼ ì‹¤í–‰í•˜ëŠ” ì—­í•  | ì¼ í•˜ëŠ” ì§ì› |
| **Web Server** | UI ì œê³µ, DAG ìƒíƒœ ëª¨ë‹ˆí„°ë§ | ëŒ€ì‹œë³´ë“œ/ëª¨ë‹ˆí„° |
| **Metadata DB** | DAG, Task ìƒíƒœ, ì‹¤í–‰ ê¸°ë¡ ì €ì¥ | ê¸°ë¡ ë³´ê´€ì†Œ |

### ì‘ë™ íë¦„

1. ê°œë°œìê°€ DAG íŒŒì¼(.py)ì„ ì‘ì„±í•´ì„œ dags í´ë”ì— ë„£ìŒ
2. **Schedulerê°€ DAG íŒŒì¼ì„ ì½ê³  íŒŒì‹±í•¨**
3. ìŠ¤ì¼€ì¤„ ì‹œê°„ì´ ë˜ë©´ Schedulerê°€ Taskë¥¼ íì— ë„£ìŒ
4. Workerê°€ íì—ì„œ Taskë¥¼ ê°€ì ¸ì™€ ì‹¤í–‰
5. ê²°ê³¼ê°€ Metadata DBì— ì €ì¥ë˜ê³  Web UIì— í‘œì‹œë¨

### íŒŒì‹±(Parsing)ì´ë€?

**íŒŒì‹± = Python íŒŒì¼ì„ ì½ì–´ì„œ "ì´í•´"í•˜ëŠ” ê³¼ì •**

Schedulerê°€ DAG íŒŒì¼ì„ ì—´ì–´ì„œ:
1. ì–´ë–¤ DAGì´ ìˆëŠ”ì§€ ì°¾ê³ 
2. ê·¸ ì•ˆì— Taskê°€ ë­ê°€ ìˆëŠ”ì§€ íŒŒì•…í•˜ê³ 
3. Task ê°„ ìˆœì„œ(ì˜ì¡´ì„±)ê°€ ì–´ë–»ê²Œ ë˜ëŠ”ì§€ ì´í•´í•˜ê³ 
4. ìŠ¤ì¼€ì¤„ì´ ë­”ì§€ í™•ì¸

> ğŸ’¡ SchedulerëŠ” ì´ íŒŒì‹±ì„ **ì£¼ê¸°ì ìœ¼ë¡œ ê³„ì†í•¨** (ê¸°ë³¸ 30ì´ˆë§ˆë‹¤). ê·¸ë˜ì„œ DAG íŒŒì¼ ìˆ˜ì •í•˜ë©´ ìë™ìœ¼ë¡œ ë°˜ì˜ë¨

---

## 4. ì‹¤ì œ ì½”ë“œ ì˜ˆì‹œ

### ê¸°ë³¸ ì˜ˆì‹œ (Task 1ê°œ)

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

# DAG ì •ì˜ (ì‘ì—… ìˆœì„œë„)
with DAG(
    dag_id='my_first_dag',
    start_date=datetime(2024, 1, 1),
    schedule='@daily'
) as dag:

    # Task ì •ì˜
    def say_hello():
        print('Hello, Airflow!')

    hello_task = PythonOperator(
        task_id='say_hello',
        python_callable=say_hello
    )
```

### ETL ì˜ˆì‹œ (Task 3ê°œ + ì˜ì¡´ì„±)

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

# DAG ì •ì˜ (ì‘ì—… ìˆœì„œë„)
with DAG(
    dag_id='my_first_dag',
    start_date=datetime(2024, 1, 1),
    schedule='@daily'
) as dag:

    # Task 1: ë°ì´í„° ìˆ˜ì§‘
    def extract_data():
        print('1ë‹¨ê³„: ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤!')
        return {'raw_data': [1, 2, 3, 4, 5]}

    # Task 2: ë°ì´í„° ì •ì œ
    def transform_data():
        print('2ë‹¨ê³„: ë°ì´í„°ë¥¼ ì •ì œí•©ë‹ˆë‹¤!')
        return {'clean_data': [2, 4, 6, 8, 10]}

    # Task 3: ê²°ê³¼ ì €ì¥
    def load_data():
        print('3ë‹¨ê³„: ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤!')
        print('ì™„ë£Œ!')

    # Task ìƒì„±
    task_extract = PythonOperator(
        task_id='extract',
        python_callable=extract_data
    )

    task_transform = PythonOperator(
        task_id='transform',
        python_callable=transform_data
    )

    task_load = PythonOperator(
        task_id='load',
        python_callable=load_data
    )

    # ì˜ì¡´ì„± ì„¤ì • (ì‹¤í–‰ ìˆœì„œ)
    # extract â†’ transform â†’ load
    task_extract >> task_transform >> task_load
```

### ì˜ì¡´ì„± ì„¤ì • ë°©ë²•

```python
# ë°©ë²• 1: >> ì—°ì‚°ì (ê°€ì¥ ë§ì´ ì”€)
task_a >> task_b >> task_c

# ë°©ë²• 2: set_downstream
task_a.set_downstream(task_b)

# ë°©ë²• 3: set_upstream (ë°˜ëŒ€ ë°©í–¥)
task_b.set_upstream(task_a)

# ë³‘ë ¬ ì‹¤í–‰ í›„ í•©ì¹˜ê¸°
#     â†’ task_b â†’
# task_a          â†’ task_d
#     â†’ task_c â†’
task_a >> [task_b, task_c] >> task_d
```

---

## 5. ì™œ Airflowë¥¼ ì“¸ê¹Œ?

### Airflowê°€ ì—†ì„ ë•Œ

- í¬ë¡ íƒ­(crontab)ìœ¼ë¡œ ìŠ¤ì¼€ì¤„ë§ â†’ ì‘ì—… ê°„ ì˜ì¡´ì„± ê´€ë¦¬ ì–´ë ¤ì›€
- ì‹¤íŒ¨ ì‹œ ì•Œë¦¼ â†’ ì§ì ‘ êµ¬í˜„í•´ì•¼ í•¨
- ì¬ì‹¤í–‰ â†’ ìˆ˜ë™ìœ¼ë¡œ í•´ì•¼ í•¨
- ëª¨ë‹ˆí„°ë§ â†’ ë¡œê·¸ íŒŒì¼ ë’¤ì ¸ì•¼ í•¨

### Airflowê°€ ìˆì„ ë•Œ

| ê¸°ëŠ¥ | ì¥ì  |
|------|------|
| ì˜ì¡´ì„± ê´€ë¦¬ | Task ê°„ ìˆœì„œë¥¼ ì½”ë“œë¡œ ëª…í™•í•˜ê²Œ ì •ì˜ |
| ìë™ ì¬ì‹œë„ | ì‹¤íŒ¨í•´ë„ ìë™ìœ¼ë¡œ ì¬ì‹œë„ ê°€ëŠ¥ |
| Web UI | ëŒ€ì‹œë³´ë“œë¡œ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ |
| ì•Œë¦¼ | Slack, Email ë“±ìœ¼ë¡œ ì•Œë¦¼ ì„¤ì • ê°€ëŠ¥ |
| ë°±í•„(Backfill) | ê³¼ê±° ë‚ ì§œ ë°ì´í„°ë„ ì‰½ê²Œ ì¬ì²˜ë¦¬ |

### ì–¸ì œ ì“°ë©´ ì¢‹ì„ê¹Œ?

- ì •í•´ì§„ ì‹œê°„ì— ìë™ìœ¼ë¡œ ì‹¤í–‰í•´ì•¼ í•˜ëŠ” ë°°ì¹˜ ì‘ì—…ì´ ìˆì„ ë•Œ
- ì—¬ëŸ¬ ì‘ì—…ì´ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰ë˜ì–´ì•¼ í•  ë•Œ (A â†’ B â†’ C)
- ETL (Extract-Transform-Load) íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•  ë•Œ
- ML ëª¨ë¸ í•™ìŠµ/ë°°í¬ë¥¼ ìë™í™”í•  ë•Œ
- ì‘ì—… ì‹¤íŒ¨ ì‹œ ì•Œë¦¼ì´ë‚˜ ìë™ ì¬ì‹œë„ê°€ í•„ìš”í•  ë•Œ

---

## 6. Operator ì‹¬í™”

### ê¸°ë³¸ ì œê³µ Operator (Built-in)

Airflow ì„¤ì¹˜í•˜ë©´ ë°”ë¡œ ì“¸ ìˆ˜ ìˆëŠ” ê²ƒë“¤:

| Operator | import ê²½ë¡œ |
|----------|-------------|
| BashOperator | `airflow.operators.bash` |
| PythonOperator | `airflow.operators.python` |
| EmailOperator | `airflow.operators.email` |
| DummyOperator | `airflow.operators.dummy` (í…ŒìŠ¤íŠ¸ìš©) |

### Provider íŒ¨í‚¤ì§€ (ì¶”ê°€ ì„¤ì¹˜ í•„ìš”)

SQL ê´€ë ¨ OperatorëŠ” DB ì¢…ë¥˜ì— ë”°ë¼ ë³„ë„ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ì•¼ í•´ìš”:

```bash
# MySQL ì“¸ ë•Œ
pip install apache-airflow-providers-mysql

# PostgreSQL ì“¸ ë•Œ
pip install apache-airflow-providers-postgres

# Snowflake ì“¸ ë•Œ
pip install apache-airflow-providers-snowflake
```

### ì£¼ìš” Provider íŒ¨í‚¤ì§€

| Provider | ìš©ë„ |
|----------|------|
| `apache-airflow-providers-google` | GCP, BigQuery, GCS |
| `apache-airflow-providers-amazon` | AWS, S3, Redshift |
| `apache-airflow-providers-slack` | Slack ì•Œë¦¼ |
| `apache-airflow-providers-http` | REST API í˜¸ì¶œ |

### Custom Operator ë§Œë“¤ê¸°

íšŒì‚¬ ë‚´ë¶€ ì‹œìŠ¤í…œ ì—°ë™ì´ë‚˜ ë°˜ë³µë˜ëŠ” ë¡œì§ì„ ì¬ì‚¬ìš©í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.

```python
from airflow.models import BaseOperator

class MyCustomOperator(BaseOperator):
    
    def __init__(self, my_param, **kwargs):
        super().__init__(**kwargs)
        self.my_param = my_param
    
    def execute(self, context):
        # ì—¬ê¸°ì— ì‹¤ì œ ë¡œì§ ì‘ì„±
        print(f"ë‚´ íŒŒë¼ë¯¸í„°: {self.my_param}")
        return "ì‘ì—… ì™„ë£Œ!"
```

> ğŸ’¡ **íŒ:** ì‹¤ë¬´ì—ì„œëŠ” ê°„ë‹¨í•œ ì‘ì—…ì€ `PythonOperator`ì— í•¨ìˆ˜ ë„£ì–´ì„œ ì“°ê³ , ì •ë§ ì—¬ëŸ¬ DAGì—ì„œ ë°˜ë³µì ìœ¼ë¡œ ì“°ì´ëŠ” ë¡œì§ë§Œ Custom Operatorë¡œ ë§Œë“œëŠ” ê²½ìš°ê°€ ë§ì•„ìš”.

---

## 7. XCom - Task ê°„ ë°ì´í„° ì „ë‹¬

### XComì´ë€?

**XCom = Task ê°„ì— ë°ì´í„°ë¥¼ ì£¼ê³ ë°›ëŠ” ë°©ë²•**

X(cross) + Com(communication) = **Task ê°„ í†µì‹ **

### ì™œ í•„ìš”í• ê¹Œ?

ê° TaskëŠ” **ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰**ë˜ê¸° ë•Œë¬¸ì— ê·¸ëƒ¥ì€ ë°ì´í„°ë¥¼ ë„˜ê¸¸ ìˆ˜ ì—†ìŒ

```python
def extract_data():
    return {'raw_data': [1, 2, 3, 4, 5]}  # ì´ ë°ì´í„°ë¥¼...

def transform_data():
    # ì—¬ê¸°ì„œ ì–´ë–»ê²Œ ë°›ì§€?? ğŸ¤”
```

### XCom ì‚¬ìš©ë²•

```python
# Task 1: ë°ì´í„° ë³´ë‚´ê¸° (push)
def extract_data(**context):
    data = [1, 2, 3, 4, 5]
    context['ti'].xcom_push(key='raw_data', value=data)
    # ë˜ëŠ” ê·¸ëƒ¥ returní•˜ë©´ ìë™ìœ¼ë¡œ pushë¨
    return data

# Task 2: ë°ì´í„° ë°›ê¸° (pull)
def transform_data(**context):
    raw_data = context['ti'].xcom_pull(task_ids='extract', key='return_value')
    print(f'ë°›ì€ ë°ì´í„°: {raw_data}')
    cleaned = [x * 2 for x in raw_data]
    return cleaned
```

### í•µì‹¬ ì •ë¦¬

| ìš©ì–´ | ì˜ë¯¸ |
|------|------|
| `xcom_push` | ë°ì´í„° ë³´ë‚´ê¸° |
| `xcom_pull` | ë°ì´í„° ê°€ì ¸ì˜¤ê¸° |
| `return` | ìë™ìœ¼ë¡œ pushë¨ |

### âš ï¸ ì£¼ì˜: DataFrame í†µì§¸ë¡œ ë„˜ê¸°ë©´ ì•ˆ ë˜ëŠ” ì´ìœ 

XComìœ¼ë¡œ ë°ì´í„°ë¥¼ ë„˜ê¸°ë©´ **Metadata DBì— ì§ì ‘ ì €ì¥**ë©ë‹ˆë‹¤!

```
Task A â†’ return df â†’ [DBì— ì €ì¥] â†’ Task Bê°€ pull â†’ [DBì—ì„œ ì½ìŒ]
```

| ë¬¸ì œ | ì„¤ëª… |
|------|------|
| DB ìš©ëŸ‰ í­ë°œ | DataFrameì´ 100MBë¼ë©´? ë§¤ì¼ ì‹¤í–‰í•˜ë©´? DB ê¸ˆë°© í„°ì§„ëŒ€ìš” |
| ì†ë„ ì €í•˜ | í° ë°ì´í„°ë¥¼ DBì— ì“°ê³  ì½ê³ ... ëŠë ¤ìš” |
| ì§ë ¬í™” ì´ìŠˆ | DataFrameì„ DBì— ì €ì¥í•˜ë ¤ë©´ ë³€í™˜(ì§ë ¬í™”)ì´ í•„ìš”í•œë°, ì´ê²ƒë„ ë¹„ìš© |

### ì˜¬ë°”ë¥¸ ë°©ë²•

```python
# âŒ ì´ë ‡ê²Œ í•˜ë©´ ì•ˆ ë¨
def extract():
    df = pd.read_csv('huge_file.csv')  # 500MBì§œë¦¬
    return df  # DBì— 500MB ì €ì¥... ëˆë‘ëŒ“

# âœ… ì´ë ‡ê²Œ í•´ì•¼ í•¨
def extract():
    df = pd.read_csv('huge_file.csv')
    path = 's3://my-bucket/data/output.parquet'
    df.to_parquet(path)
    return path  # ê²½ë¡œë§Œ ë„˜ê¹€ (ëª‡ ê¸€ì)

def transform(**context):
    path = context['ti'].xcom_pull(task_ids='extract')
    df = pd.read_parquet(path)  # S3ì—ì„œ ì§ì ‘ ì½ìŒ
```

---

## ì°¸ê³  ìë£Œ

- [Apache Airflow ê³µì‹ ë¬¸ì„œ](https://airflow.apache.org/docs/)
- [Airflow Provider íŒ¨í‚¤ì§€ ëª©ë¡](https://airflow.apache.org/docs/apache-airflow-providers/)
