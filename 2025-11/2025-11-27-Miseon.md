# Attention Is All You Need - 1

> 원문: https://arxiv.org/pdf/1706.03762

---

## 1. 기존 모델의 흐름

기존 시퀀스 모델(RNN 등)의 처리 방식:

```
input → hidden → h → ... → output
```

### 장기 의존성 문제 (Long-term Dependency)

h가 계속 전달되면서 **먼 거리의 정보가 소실**되는 문제 발생
문장이 길어질수록 앞쪽 정보를 기억하기 어려움

### Transformer의 해결책

Self-Attention으로 모든 위치를 직접 참조
거리에 상관없이 어떤 토큰이든 직접 연결 가능 → 장기 의존성 문제 해결

---

## 2. Transformer 구조

### Input

```
Input Embedding + Positional Encoding → 위치 정보가 포함됨
```

### Self-Attention Layer

- **나 자신을 포함한 모든 토큰**에 대해 attention 메커니즘 적용
- ex. '는' 토큰은 나/는/바보다 에 모두 주의를 기울일 수 있음

### Feed-Forward Layer

- Self-Attention 이후 추가되는 레이어
- 문맥 벡터 생성됨

---

## 3. Positional Encoding (위치 인코딩)
> 원래 임베딩 벡터에 위치 정보를 더해줌

### 수식

$$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$

$$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$

- `pos`: 토큰의 위치
- `i`: 차원 인덱스
- `d_model`: 모델의 차원 수

### sin, cos를 사용하는 이유

**문장이 얼마나 길어지든 위치 벡터 생성 가능!**

삼각함수 덧셈 정리를 활용:

$$PE_{(pos+k)} = \sin(pos+k) = \sin(pos)\cos(k) + \cos(pos)\sin(k)$$

- 고정된 k에 대해, `pos + k` 위치의 인코딩을 **pos 위치의 인코딩으로부터 선형 변환**으로 표현 가능
- 이를 통해 모델이 **상대적 위치 관계**를 학습할 수 있음

---

## 핵심 정리

| 구성요소 | 역할 |
|---------|------|
| Input Embedding | 토큰을 벡터로 변환 |
| Positional Encoding | 위치 정보 추가 |
| Self-Attention | 토큰 간 관계 계산 |
| Feed-Forward | 비선형 변환 적용 |